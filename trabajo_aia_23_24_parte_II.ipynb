{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDitZsQALa1H"
      },
      "source": [
        "# TRABAJO Parte 2: AIA_2022-2023"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nombre y DNI del alumno/a 1: Santiago Miñarro García\n"
      ],
      "metadata": {
        "id": "K3yRPoVAo6vO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C0VZXpaWwBZ"
      },
      "source": [
        "# Transfer Learning con CNNs - Dataset: Flowers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX-nPhwRLuBL"
      },
      "source": [
        "La idea de este trabajo es familiarizarnos con dos situaciones muy habituales en la actividad real de un \"Machine Learning Engineer\":\n",
        "\n",
        "1.   En primer lugar, con una de las técnicas más potentes asociadas con las redes neuronales: el **Transfer Learning**. Dado que las redes neuronales, para resolver un problema, capturan en su estructura de capas y pesos una representación jerárquica del problema.\n",
        "Entonces..., ¿por que no aprovechar ese conocimiento obtenido, para resolver otro problema diferente?\n",
        "\n",
        "2.  En segundo lugar, con la **busqueda de información sobre conceptos nuevos**. En este caso, los dos primeros modelos a implementar los hemos trabajado en clase. No así el Transfer Learning, y por tanto, debereis buscar vosotros mismos como hacer lo que se pide para el Modelo 3. Consultar en blogs, web y tutoriales es algo común en el día a día de alguien que quiere profundizar en el ML y, para ello, existen infinidad de fuentes. A modo de ejemplo, una fuente para profundizar en el Transfer Learning con redes convolucionales es: https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/\n",
        "\n",
        "En este trabajo vamos intentar resolver un problema de clasificación sobre un dataset propuesto por Tensorflow en 2019 conocido como \"flowers\". Este conjunto está formado por 3670 imágenes de flores pertenecientes a 5 clases diferentes. Para ello implementaremos 3 modelos:\n",
        "\n",
        "*   Modelo 1: implementación de una CNN básica.\n",
        "*   Modelo 2: es una evolución del modelo anterior, aplicando técnicas que reduzcan el overfitting.\n",
        "*   Modelo 3: rompemos la barrera de tener que seguir complicando nuestro modelo y se pide aplicar transfer learning utilizando un pre-trained model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PUYhaYNf9M0"
      },
      "source": [
        "# a) Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B9DNPpbPPTFN",
        "outputId": "26e1bdc9-d5c5-440d-c364-66c7c858351e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pathlib\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLZg4N3QqKtJ"
      },
      "source": [
        "Descargamos el dataset que pone a nuestra disposición Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RvfQsk5kGeXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cca6e83-236c-48d7-a1d0-266be6683584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "228813984/228813984 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "\n",
        "zip_file = tf.keras.utils.get_file(origin=_URL,\n",
        "                                   fname=\"flower_photos.tgz\",\n",
        "                                   extract=True)\n",
        "\n",
        "base_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR636nTLHbAr"
      },
      "source": [
        "Tras completar la descarga, debemos tener la siguiente estructura de directorios:  \n",
        "\n",
        "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
        "<b>flower_photos</b>\n",
        "|__ <b>daisy</b>\n",
        "|__ <b>dandelion</b>\n",
        "|__ <b>roses</b>\n",
        "|__ <b>sunflowers</b>\n",
        "|__ <b>tulips</b>\n",
        "</pre>\n",
        "\n",
        "Desgraciadamente, para este dataset, Tensorflow no nos proporciona la estructura de directorios necesaria de train y de validación. Por lo que debemos proceder del siguiente modo:\n",
        "\n",
        "* Crear una carpeta `train` y de `val`, cada una de ellas debe contener a su vez, cinco subdirectorios: uno para cada clase de flor.\n",
        "* Moveremos las imágenes de las carpetas originales a estas nuevas carpetas. De modo que el 80% de las imágenes vayan al conjunto de train y el 20% restante al de validación.\n",
        "* La estructura final de directorios debe ser la siguiente:\n",
        "\n",
        "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
        "<b>flower_photos</b>\n",
        "|__ <b>train</b>\n",
        "    |______ <b>daisy</b>: [12.jpg, 28.jpg, 31.jpg ....]\n",
        "    |______ <b>dandelion</b>: [41.jpg, 22.jpg, 35.jpg ....]\n",
        "    |______ <b>roses</b>: [121.jpg, 92.jpg, 38.jpg ....]\n",
        "    |______ <b>sunflowers</b>: [93.jpg, 23.jpg, 83.jpg ....]\n",
        "    |______ <b>tulips</b>: [109.jpg, 267.jpg, 93.jpg ....]\n",
        " |__ <b>val</b>\n",
        "    |______ <b>daisy</b>: [507.jpg, 508.jpg, 509.jpg ....]\n",
        "    |______ <b>dandelion</b>: [719.jpg, 720.jpg, 721.jpg ....]\n",
        "    |______ <b>roses</b>: [514.jpg, 515.jpg, 516.jpg ....]\n",
        "    |______ <b>sunflowers</b>: [560.jpg, 561.jpg, 562.jpg .....]\n",
        "    |______ <b>tulips</b>: [640.jpg, 641.jpg, 642.jpg ....]\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTVvGj2tZnIH"
      },
      "source": [
        "Creamos una lista con el nombre de las 5 clases. En castellano sería: margaritas, diente de león, rosas, girasoles y tulipanes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "60aGWgMlZpTJ"
      },
      "outputs": [],
      "source": [
        "classes = ['roses', 'daisy', 'dandelion', 'sunflowers', 'tulips']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1PyDGL1aDCj"
      },
      "source": [
        "Creemos la estructura de directorios necesaria:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYGPAriDXS2d",
        "outputId": "129e9887-9343-4ab8-8dae-1938ad56bc50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roses: 641 Imagenes\n",
            "daisy: 633 Imagenes\n",
            "dandelion: 898 Imagenes\n",
            "sunflowers: 699 Imagenes\n",
            "tulips: 799 Imagenes\n"
          ]
        }
      ],
      "source": [
        "SPLIT_RATIO=0.8\n",
        "\n",
        "for cl in classes:\n",
        "    # path de las imagenes de la clase cl\n",
        "    img_path = os.path.join(base_dir, cl)\n",
        "\n",
        "    # obtenemos la lista de todas las imagenes\n",
        "    images = glob.glob(img_path + '/*.jpg')\n",
        "    print(\"{}: {} Imagenes\".format(cl, len(images)))\n",
        "\n",
        "    # determinamos cuantas imagenes son el 80%\n",
        "    num_train = int(round(len(images)*SPLIT_RATIO))\n",
        "\n",
        "    # separamos las imagenes en dos listas\n",
        "    train, val = images[:num_train], images[num_train:]\n",
        "\n",
        "    # creamos la carpeta de train/clase y val/clase\n",
        "    if not os.path.exists(os.path.join(base_dir, 'train', cl)):\n",
        "        os.makedirs(os.path.join(base_dir, 'train', cl))\n",
        "    else:\n",
        "        shutil.rmtree(os.path.join(base_dir, 'train', cl))\n",
        "\n",
        "    if not os.path.exists(os.path.join(base_dir, 'val', cl)):\n",
        "        os.makedirs(os.path.join(base_dir, 'val', cl))\n",
        "    else:\n",
        "        shutil.rmtree(os.path.join(base_dir, 'val', cl))\n",
        "\n",
        "    for t in train:\n",
        "        shutil.move(t, os.path.join(base_dir, 'train', cl))\n",
        "\n",
        "    for v in val:\n",
        "        shutil.move(v, os.path.join(base_dir, 'val', cl))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htR3aJ2QXaxx"
      },
      "source": [
        "Preparamos variables con las rutas de los diferentes directorios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3R1AhKJdb87G"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "daisy_dir = os.path.join(train_dir, 'daisy')\n",
        "dandelion_dir = os.path.join(train_dir, 'dandelion')\n",
        "roses_dir = os.path.join(train_dir, 'roses')\n",
        "sunflowers_dir = os.path.join(train_dir, 'sunflowers')\n",
        "tulips_dir = os.path.join(train_dir, 'tulips')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh6vmp4nQyxn"
      },
      "source": [
        "**Tarea 1: Muestre el nombre de dos ficheros cualquiera en alguna de esas rutas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgpsenXRNLzV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "580495ef-e29b-4345-a881-f7a370d10d4e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "[Errno 20] Not a directory: '/root/.keras/datasets/flower_photos/train/daisy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d76593ab8b19>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# El código aquí\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marchivos_daisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaisy_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchivos_daisy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marchivos_daisy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaisy_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/root/.keras/datasets/flower_photos/train/daisy'"
          ]
        }
      ],
      "source": [
        "# El código aquí\n",
        "archivos_daisy = os.listdir(daisy_dir)\n",
        "print(archivos_daisy[0],archivos_daisy[1])\n",
        "print(daisy_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP0qLFsUwr3R"
      },
      "source": [
        "Es decir, la clase a la que pertenece cada imagen no viene dada por el nombre del fichero sino por el directorio en el que se encuentra almacenada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvdoqIPGxNO3"
      },
      "source": [
        "**Tarea 2: Muestra el número de imágenes de train que tenemos de cada clase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gk4dSZgNq6o"
      },
      "outputs": [],
      "source": [
        "# El código aquí\n",
        "for i in os.listdir(train_dir):\n",
        "  print(\"Clase \" + i + \": \" + str(len(os.listdir(train_dir + \"/\" + i ))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXiUqkuHx3j0"
      },
      "source": [
        "# b) Visualización del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMpndtLqLfN1"
      },
      "source": [
        "**Tarea 3: Muestra 3 imágenes de cada una de las clases, el título de la imagen será el shape del array de numpy asociado a la imagen**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyGOdClOMaMm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "plt.figure(figsize=(16,12))\n",
        "subplot_contador = 1\n",
        "\n",
        "for categoria in os.listdir(train_dir):\n",
        "    dir_categoría = os.path.join(train_dir, categoria)\n",
        "    archivos = os.listdir(dir_categoría)\n",
        "\n",
        "    for j in range(3):\n",
        "        if j < len(archivos):\n",
        "            ruta_imagen = os.path.join(dir_categoría, archivos[j])\n",
        "            imagen = mpimg.imread(ruta_imagen)\n",
        "            ax = plt.subplot(len(os.listdir(train_dir)), 3, subplot_contador)\n",
        "            ax.axis('off')\n",
        "            ax.imshow(imagen)\n",
        "            ax.set_title(f\"{categoria} - {j+1}\")\n",
        "            subplot_contador += 1\n",
        "\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsjyjYWC0El4"
      },
      "source": [
        "# c) Modelo 1: CNN básica (objetivo: accuracy_valid > 60%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUThvU0o0Yan"
      },
      "source": [
        "Implemente una red convolucional para resolver el problema de clasificación. Para ello se sugiere una CNN con 3 capas convolucionales + pooling con la siguiente estructura:\n",
        "\n",
        "Bloque de procesamiento de imagen:\n",
        "1.   32 kernels -> 64 kernels -> 96 kernels\n",
        "2.   kernels de 3x3.\n",
        "3.   Stride = 1 y padding = SI.\n",
        "4.   Función de activación ReLU.\n",
        "5.   Maxpooling de 2x2 con stride clásico de 2 pixeles.\n",
        "6.   Igualamos el tamaño de todas las imágenes a 150 x 150.\n",
        "\n",
        "Bloque de decisión:\n",
        "7.   Capa densa de 512 neuronas.\n",
        "8.   Capa densa de salida.\n",
        "\n",
        "**Tarea 4: Define un modelo con la estructura anterior**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYLc-4hlWteF"
      },
      "outputs": [],
      "source": [
        "# El código aquí\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Conv2D(\n",
        "              filters=32,\n",
        "              kernel_size=(3, 3),\n",
        "              strides=(1, 1),\n",
        "              padding='same',\n",
        "              activation='relu',\n",
        "              name='conv_1',\n",
        "              input_shape=(150, 150, 3)))\n",
        "\n",
        "\n",
        "model.add(keras.layers.MaxPooling2D(\n",
        "              pool_size=(2, 2),\n",
        "              name='Pool_2',\n",
        "              strides=(2, 2)))\n",
        "\n",
        "model.add(keras.layers.Conv2D(\n",
        "              filters=64,\n",
        "              kernel_size=(3, 3),\n",
        "              strides=(1, 1),\n",
        "              padding='same',\n",
        "              activation='relu',\n",
        "              name='conv_2',\n",
        "              input_shape=(75, 75, 32)))\n",
        "\n",
        "model.add(keras.layers.MaxPooling2D(\n",
        "              pool_size=(2, 2),\n",
        "              name='Pool_1',\n",
        "              strides=(2, 2)))\n",
        "\n",
        "model.add(keras.layers.Conv2D(\n",
        "              filters=64,\n",
        "              kernel_size=(3, 3),\n",
        "              strides=(1, 1),\n",
        "              padding='same',\n",
        "              activation='relu',\n",
        "              name='conv_3',\n",
        "              input_shape=(37, 37, 64)))\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "\n",
        "model.add(keras.layers.Dense(units=512,name='densa_1', activation='relu'))\n",
        "model.add(keras.layers.Dense(units=1, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 5: Indica el shape de la imagen antes y después de cada capa de la red. Explica cómo has obtenido dichos valores**"
      ],
      "metadata": {
        "id": "V28tWVRj4XnU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb6ZtlqM6tC6"
      },
      "source": [
        "|Capa| Shape a la salida| #parámetros |\n",
        "|:-|:-:|:-:|\n",
        "|Conv_1|150 x 150 x 32|\n",
        "|Pool_1| 75 x 75 x 32|\n",
        "|Conv_2| 75 x 75 x 64|\n",
        "|Pool_2| 37 x 37 x 64|\n",
        "|flatten| 87616 |\n",
        "|densa_1| 1 |\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformación de Dimensiones en una Red Neuronal Convolucional\n",
        "\n",
        "Primera Capa - Conv_1:\n",
        "\n",
        "Input Shape: 150×150×3\n",
        "\n",
        "Esta es la forma de las imágenes ingresadas: 150x150 píxeles con 3 canales de color (RGB).\n",
        "\n",
        "Parámetros de la Capa:\n",
        "\n",
        "Filtros: 32\n",
        "\n",
        "Tamaño del Kernel:\n",
        "3\n",
        "×\n",
        "3\n",
        "\n",
        "Stride:\n",
        "(1\n",
        ",\n",
        "1)\n",
        "\n",
        "Padding: 'same'\n",
        "\n",
        "Output Shape: Debido al padding 'same' y al stride de (1,1), la dimensión espacial de la salida permanece igual pero el número de canales cambia al número de filtros. Por lo tanto, el shape de salida es\n",
        "150\n",
        "×\n",
        "150\n",
        "×\n",
        "32\n",
        "\n",
        "Segunda Capa - Pool_1:\n",
        "\n",
        "Input Shape:\n",
        "150\n",
        "×\n",
        "150\n",
        "×\n",
        "32\n",
        "\n",
        "Parámetros de la Capa:\n",
        "\n",
        "Tamaño del Pool:\n",
        "2\n",
        "×\n",
        "2\n",
        "\n",
        "Stride:\n",
        "(2\n",
        ",\n",
        "2)\n",
        "\n",
        "Output Shape: El pooling reduce cada dimensión espacial a la mitad cuando el tamaño del pool es 2 y el stride es 2. Por lo tanto, el shape de salida es\n",
        "75\n",
        "×\n",
        "75\n",
        "×\n",
        "32\n",
        "\n",
        "Tercera Capa - Conv_2:\n",
        "\n",
        "Input Shape:\n",
        "75\n",
        "×\n",
        "75\n",
        "×\n",
        "32\n",
        "\n",
        "Parámetros de la Capa:\n",
        "\n",
        "Filtros: 64\n",
        "\n",
        "Tamaño del Kernel:\n",
        "3\n",
        "×\n",
        "3\n",
        "\n",
        "\n",
        "Stride:\n",
        "(1\n",
        ",\n",
        "1)\n",
        "\n",
        "Padding: 'same'\n",
        "\n",
        "Output Shape: Con los mismos parámetros de padding y stride, el tamaño espacial se mantiene, cambiando solo el número de canales a 64. El shape de salida es\n",
        "75\n",
        "×\n",
        "75\n",
        "×\n",
        "64\n",
        "\n",
        "Cuarta Capa - Pool_2:\n",
        "\n",
        "Input Shape:\n",
        "75\n",
        "×\n",
        "75\n",
        "×\n",
        "64\n",
        "\n",
        "Parámetros de la Capa:\n",
        "\n",
        "Tamaño del Pool:\n",
        "2\n",
        "×\n",
        "2\n",
        "\n",
        "\n",
        "Stride:\n",
        "(2\n",
        ",\n",
        "2)\n",
        "\n",
        "Output Shape: Similar al primer MaxPooling, el tamaño espacial se reduce a la mitad. Por lo tanto, el shape de salida es\n",
        "37\n",
        "×\n",
        "37\n",
        "×\n",
        "64\n",
        "\n",
        "Quinta Capa - Conv_3:\n",
        "\n",
        "Input Shape:\n",
        "37\n",
        "×\n",
        "37\n",
        "×\n",
        "64\n",
        "\n",
        "Parámetros de la Capa:\n",
        "\n",
        "Filtros: 64\n",
        "\n",
        "Tamaño del Kernel:\n",
        "3\n",
        "×\n",
        "3\n",
        "\n",
        "Stride:\n",
        "(1\n",
        ",\n",
        "1)\n",
        "\n",
        "Padding: 'same'\n",
        "\n",
        "Output Shape: Manteniendo el padding 'same', el tamaño espacial no cambia y el número de canales sigue siendo 64. El shape de salida es\n",
        "\n",
        "37 × 37 × 64"
      ],
      "metadata": {
        "id": "o848fkke3-ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 6: Compara el resultado con un summary() del modelo**\n"
      ],
      "metadata": {
        "id": "kZJMAJfN4zIR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS4FCNLWWwFu"
      },
      "outputs": [],
      "source": [
        "# el código aquí\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 7: Entrena el modelo de manera que obtenga un accuracy (sobre el conjunto de validación) > 60%.**\n",
        "\n",
        "* Utilice el optimizador que considere más adecuado.\n",
        "*   Recuerda que si no se realiza conversión a One-Hot de la etiqueta a predecir, debes utilizar como función de error `SparseCategoricalCrossentropy` (este es el procedimiento que hemos usado en clase).\n",
        "*   Considera un learning rate en el entorno de 0.001.\n",
        "*   En el caso de los generators utiliza `class_mode='sparse'`.\n",
        "*   Puedes utilizar p.e. un `batch_size = 100`."
      ],
      "metadata": {
        "id": "S7XxJcrg4K5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=100,\n",
        "        class_mode='sparse')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=100,\n",
        "        class_mode='sparse')\n"
      ],
      "metadata": {
        "id": "bYOBjL5g7JYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1ONBmzLkWwS-"
      },
      "outputs": [],
      "source": [
        "# El código aquí\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "es_callback = keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                    batch_size=20,\n",
        "                    epochs=15,\n",
        "                    validation_data=validation_generator,\n",
        "                    callbacks=[es_callback])"
      ],
      "metadata": {
        "id": "J35wN7p3-MPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPfDU2p0WfCw"
      },
      "source": [
        "**Tarea 8: Muestra la evolución de la función de error (train y valid) durante el entrenamiento.Explica que problema presenta el modelo que hemos entrenado.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwrinxkCYhxw"
      },
      "outputs": [],
      "source": [
        "# El código aquí\n",
        "pd.DataFrame({'loss_train': history.history['loss'],\n",
        "              'loss_valid': history.history['val_loss']}).plot(figsize=(8,4))\n",
        "\n",
        "plt.grid(True)\n",
        "plt.ylim(0,1)\n",
        "plt.xlabel('numero de epocas')\n",
        "plt.title('Training and validation loss');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoYA2oVr4gkz"
      },
      "source": [
        "# d) Modelo 2: reducción del overfitting (objetivo: accuracy_valid > 70%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok65EUWQCU_q"
      },
      "source": [
        "Para mejorar el accuracy del modelo, vamos a incorporar las dos técnicas más habituales de reducción del overfitting:\n",
        "\n",
        "    * drop-out\n",
        "    * data augmentation\n",
        "    \n",
        "\n",
        "**Tarea 9: Explica en que consisten y qué utilidad tienen para nuestro problema**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### La explicación aqui"
      ],
      "metadata": {
        "id": "jFw8epb88yTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop-out es la técnica que consiste en \"apagar\" un porcentaje de neuronas lo que permite esencialmente que la red sea menos propensa a memorizar detalles pequeños y específicos de las imágenes con las que se entrena. Esto ayuda a que la red sea más flexible y mejor en reconocer imágenes nuevas y diferentes, no solo las que ya ha visto durante el entrenamiento.\n",
        "\n",
        "Data Augmentation consiste en aumentar el número de ejemplos sobre el que tu modelo aprende modificando ligeramente los datos de entrenamiento para crear nuevos de los cuales el modelo pueda aprender. Esto permite que el modelo aprenda a reconocer flores desde diferentes perspectivas y condiciones, aumentando su capacidad de generalizar bien para cuando se enfrente a una nueva imagen que no se encuentra en el conjunto de entrenamiento."
      ],
      "metadata": {
        "id": "NQ8D1O3dsRDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 10: Construye un nuevo modelo 2 incorporando (en el modelo 1 anterior) el dropout adecuado**"
      ],
      "metadata": {
        "id": "s9amUdrG7x48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbT01-lpddH2"
      },
      "outputs": [],
      "source": [
        "model2 = keras.Sequential()\n",
        "model2.add(keras.layers.Conv2D(\n",
        "              filters=32,\n",
        "              kernel_size=(3, 3),\n",
        "              strides=(1, 1),\n",
        "              padding='same',\n",
        "              activation='relu',\n",
        "              name='conv_1',\n",
        "              input_shape=(150, 150, 3)))\n",
        "\n",
        "\n",
        "model2.add(keras.layers.MaxPooling2D(\n",
        "              pool_size=(2, 2),\n",
        "              name='Pool_2',\n",
        "              strides=(2, 2)))\n",
        "\n",
        "model2.add(keras.layers.Conv2D(\n",
        "              filters=64,\n",
        "              kernel_size=(3, 3),\n",
        "              strides=(1, 1),\n",
        "              padding='same',\n",
        "              activation='relu',\n",
        "              name='conv_2',\n",
        "              input_shape=(75, 75, 32)))\n",
        "\n",
        "model2.add(keras.layers.MaxPooling2D(\n",
        "              pool_size=(2, 2),\n",
        "              name='Pool_1',\n",
        "              strides=(2, 2)))\n",
        "\n",
        "model2.add(keras.layers.Conv2D(\n",
        "              filters=64,\n",
        "              kernel_size=(3, 3),\n",
        "              strides=(1, 1),\n",
        "              padding='same',\n",
        "              activation='relu',\n",
        "              name='conv_3',\n",
        "              input_shape=(37, 37, 64)))\n",
        "\n",
        "\n",
        "model2.add(tf.keras.layers.Flatten())\n",
        "\n",
        "\n",
        "model2.add(keras.layers.Dense(units=512,name='densa_1', activation='relu'))\n",
        "model2.add(keras.layers.Dropout(0.5))  #Dropout\n",
        "model2.add(keras.layers.Dense(units=1, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Y9AJx6-ISZ"
      },
      "source": [
        "**Tarea 11: Explica que tipos de augmentation vas a considerar y que utilidad tienen en nuestro problema de clasificacion**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### La explicación aqui"
      ],
      "metadata": {
        "id": "jaFE9z8T-Wqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a considerar los siguientes tipos:\n",
        "\n",
        "Rotaciones -> Añadiremos leves rotaciones a las imágenes para que reflejen una posible realidad en la que la foto esta inclinada hacia la izquierda o hacia la derecha.\n",
        "\n",
        "Desplazamientos -> Mover las imágenes horizontal o verticalmente para que el modelo pueda reconocer fotos en las que las flores están mal centradas.\n",
        "\n",
        "Reflejo: Invertir las imágenes horizontalmente para aumentar el número de posibilidades.\n",
        "\n",
        "Zoom -> Añadiremos zoom para reflejar la situación real en la que la flor se vea de mas cerca.\n"
      ],
      "metadata": {
        "id": "UZnbjL6HzZOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen1 = ImageDataGenerator(\n",
        "#    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "val_datagen1 = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "A6ciuP4I3Dof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 12: Entrena el modelo de manera que obtenga un accuracy (sobre el conjunto de validación) > 70%.**"
      ],
      "metadata": {
        "id": "ip7dUHFK-a9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El código aquí\n",
        "train_generator1 = train_datagen1.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='sparse')\n",
        "\n",
        "validation_generator1 = val_datagen1.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='sparse')\n",
        "\n",
        "model2.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=['acc'])\n",
        "\n",
        "es_callback = keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True)\n",
        "\n",
        "\n",
        "history2 = model2.fit(train_generator1,\n",
        "                    batch_size=20,\n",
        "                    epochs=15,\n",
        "                    validation_data=validation_generator1,\n",
        "                    callbacks=[es_callback])"
      ],
      "metadata": {
        "id": "xKgb3LZK-MG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 13: Muestra la evolucion de la funcion de error durante el entrenamiento. Explica qué diferencias de comportamiento hay entre las gráficas del modelo 1 y el modelo 2**"
      ],
      "metadata": {
        "id": "O_eAQ33JCqcr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oMupPC2dT4f"
      },
      "outputs": [],
      "source": [
        "# El código aquí\n",
        "pd.DataFrame({'loss_train': history2.history['loss'],\n",
        "              'loss_valid': history2.history['val_loss']}).plot(figsize=(8,4))\n",
        "\n",
        "plt.grid(True)\n",
        "plt.ylim(0,1)\n",
        "plt.xlabel('numero de epocas')\n",
        "plt.title('Training and validation loss');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLlScafUEH-J"
      },
      "source": [
        "# c) Modelo 3: Transfer Learning (objetivo: accuracy_valid > 88%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wct4sXqnGma1"
      },
      "source": [
        "En nuestro problema de clasificación de flores, el utilizar CNNs diseñadas y entrenadas por nosotros mismos, aparece una barrera en las proximidades del 75-77% de precisión del modelo (siempre sobre validación).\n",
        "\n",
        "Para superar este escollo, el siguiente paso natural es la utilización de modelos preentrenados. Existe una gran variedad de ellos basados en redes CNNs clásicas, donde la principal diferencia es que acumulan más capas que nuestros modelos 1 y 2. Adicionalmente, en estos modelos se han ido incorporando diferentes propuestas para mejorar la arquitectura de la CNN.\n",
        "\n",
        "En general, utilizando estos modelos convolucionales preentrenados podemos alcanzar accuracies próximos al 90%. Normalmente, estos modelos han sido previamente entrenados sobre datasets de gran tamaño y con gran número de categorías. P.e. en subconjuntos de Imagenet (14 millones de imagenes de 22K categorías).\n",
        "\n",
        "Dado que estos modelos se entrenaron para resolver un problema \"relativamente\" parecido a nuestro problema de clasificación, parece razonable pensar que podemos aprovechar ese conocimiento capturado en la red para resolver nuestro problema de clasificación de flores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para elegir el modelo preentrenado que debe utilizar cada grupo, proceda del siguiente modo:\n",
        "* Paso 1: sume los DNIs de los componentes del grupo (si el grupo tiene un sólo miembro, vaya directamente al paso 2). res = dni_1 + dn_2\n",
        "* Paso 2: Aplique la siguiente operación al resultado anterior: res mod 6.\n",
        "* Paso 3: Tome el modelo cuyo número asociado coincide con el resultado de la operación anterior.\n",
        "* Paso 4: El porcentaje que aparece entre paréntesis junto al nombre del modelo es el accuracy (en validación) que deberías poder alcanzar sin dificultad utilizando el modelo. En todos los casos considera un input_shape = (224, 224, 3).\n",
        "\n",
        "  0. Resnet50 (>90%)\n",
        "  https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50\n",
        "\n",
        "  1. Resnet101 (>90%) input_shape = (224, 224, 3)\n",
        "  https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/ResNet101\n",
        "\n",
        "  2. VGG16 (>90%) input_shape = (224, 224, 3)\n",
        "  https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16\n",
        "\n",
        "  3. VGG19 (>90%) input_shape = (224, 224, 3) https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg19/VGG19\n",
        "\n",
        "  4. Xception (>88%) input_shape = (224, 224, 3) https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception/Xception\n",
        "\n",
        "  5. Inceptionv3 (>88%) input_shape = (224, 224, 3) https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/InceptionV3\n",
        "\n",
        "P.e. si los DNIs de los alumnos son: 12345678 y 23456781. La suma es 35802459. De donde 35802459 mod 6 = 3. Por tanto, tomaríamos el modelo VGG16.\n",
        "\n",
        "Recuerda que puedes utilizar el siguiente post como referencia del uso de transfer learning: https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/"
      ],
      "metadata": {
        "id": "pEv-W62PE_Uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 14: Importa el modelo desde Tensorflow**\n",
        "\n",
        "Dado que este modelo ha sido entrenado para clasificar entre 1.000 categorías, las capas densas finales del modelo no son útiles para nuestro problema de clasificación de 5 categorías (es lo que suele llamarse include_top = SI/NO). De manera que eliminamos lo que a veces se suele llamar el \"top model\". De este modo sólo nos quedamos con la parte que hace la funcionalidad de \"procesamiento\" de la imagen."
      ],
      "metadata": {
        "id": "yhVjwjvEG9Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El código aquí\n",
        "from keras.applications.xception import Xception,preprocess_input\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "S5mzgkvjHPYV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 15: Personalizar el bloque de decisión**  \n",
        "Añadimos una capa de flatten y tres nuevas capas densas especificas para nuestro problema con dimensiones 4096, 1072 y la que necesite la capa de salida (con sus correspondientes drop-outs)."
      ],
      "metadata": {
        "id": "3DEx2Wt7EwMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El código aquí\n",
        "\n",
        "input_shape =(224,224,3)\n",
        "n_clases = 5\n",
        "\n",
        "\n",
        "conv_base = Xception(include_top = False,input_shape = input_shape)\n",
        "top_model = conv_base.output\n",
        "top_model = keras.layers.Flatten(name=\"flatten\")(top_model)\n",
        "top_model = keras.layers.Dense(4096, activation='relu')(top_model)\n",
        "top_model = keras.layers.Dense(1072, activation='relu')(top_model)\n",
        "top_model = keras.layers.Dropout(0.2)(top_model)\n",
        "output_layer = keras.layers.Dense(n_clases,activation='softmax')(top_model)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xQjE646eIIRL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 16: Congelar los pesos que no se vayan a entrenar**\n",
        "\n",
        "Previo a hacer el denominado `Fine-Tuning` del modelo, indicaremos a Tensorflow que únicamente debe entrenar:\n",
        "\n",
        "   * Las dos últimas capas convolucionales de la red preentrenada, de las que realizaremos un ajuste fino de los pesos.\n",
        "   * Las tres capas densas que hemos incluido nuevas."
      ],
      "metadata": {
        "id": "4P_FzD1YIMwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El código aquí\n",
        "for layer in conv_base.layers[:-2]:\n",
        "            layer.trainable = False"
      ],
      "metadata": {
        "id": "ogbqIwMlJQ03"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 17: Crear los datagenerators oportunos**\n",
        "\n",
        "Para ello:\n",
        "> * Utiliza Data augmentation.  \n",
        "> * Las imágenes tienen que ser preprocesadas igual que cuando se entrenó el modelo pre.entrenado original. Para ello se utiliza el parámetro `preprocessing_function=preprocess_input` (preprocess_input importado desde `keras.applications.xxxxxx` en ambos generators (train y valid). Por tanto, no hay que indicarle `rescale`. En caso de ser necesario, se encargará `preprocess_input`.  \n",
        "> * Dado que estamos reutilizando un modelo que no \"es nuestro\", deberemos ceñirnos al tamaño de imagen que permite la red a la entrada. Recuerda que debe ser: 224x224."
      ],
      "metadata": {
        "id": "HWuP_cVFHIUA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eQ96DlIbN3eh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01550dfa-e22c-44c6-af13-4be62a574a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2935 images belonging to 5 classes.\n",
            "Found 735 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "# El codigo aquí\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_generator = ImageDataGenerator(rotation_range=40,\n",
        "                                    width_shift_range=0.2,\n",
        "                                    height_shift_range=0.2,\n",
        "                                    shear_range=0.2,\n",
        "                                    zoom_range=0.2,\n",
        "                                    horizontal_flip=True,\n",
        "                                    validation_split=0.15,\n",
        "                                    preprocessing_function=preprocess_input)\n",
        "\n",
        "traingen = train_generator.flow_from_directory(train_dir,\n",
        "                                               target_size=(224, 224),\n",
        "                                                    batch_size=100,\n",
        "                                                    class_mode='sparse')\n",
        "\n",
        "validgen = train_generator.flow_from_directory(val_dir,\n",
        "                                               target_size=(224, 224),\n",
        "                                                batch_size=100,\n",
        "                                                class_mode='sparse')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tarea 18: Haz el fine-tuning del modelo con el objetivo de alcanzar un accuracy (sobre el conjunto de validación > 88%).**\n",
        "\n",
        "A la hora de entrenar un modelo pretrained es típico bajar el learning rate respecto al que utilizaríamos para un modelo nuestro desde cero."
      ],
      "metadata": {
        "id": "Ai89loLzKiI7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "foqUUofHOrdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607c2826-bc38-4dd1-bbe4-4c50913b5a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 1.5549 - accuracy: 0.5970"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 58s 4s/step - loss: 1.5549 - accuracy: 0.5970 - val_loss: 1.1789 - val_accuracy: 0.6912\n",
            "Epoch 2/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.8132 - accuracy: 0.7519"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 15s 1s/step - loss: 0.8132 - accuracy: 0.7519\n",
            "Epoch 3/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.8080"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 16s 2s/step - loss: 0.5569 - accuracy: 0.8080\n",
            "Epoch 4/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4788 - accuracy: 0.8390"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 16s 2s/step - loss: 0.4788 - accuracy: 0.8390\n",
            "Epoch 5/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4113 - accuracy: 0.8470"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 16s 2s/step - loss: 0.4113 - accuracy: 0.8470\n",
            "Epoch 6/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.8450"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 17s 2s/step - loss: 0.4741 - accuracy: 0.8450\n",
            "Epoch 7/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4350 - accuracy: 0.8481"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 15s 1s/step - loss: 0.4350 - accuracy: 0.8481\n",
            "Epoch 8/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8299"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 15s 1s/step - loss: 0.4298 - accuracy: 0.8299\n",
            "Epoch 9/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3655 - accuracy: 0.8730"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 16s 2s/step - loss: 0.3655 - accuracy: 0.8730\n",
            "Epoch 10/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8717"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 15s 1s/step - loss: 0.3548 - accuracy: 0.8717\n",
            "Epoch 11/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3527 - accuracy: 0.8730"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 16s 2s/step - loss: 0.3527 - accuracy: 0.8730\n",
            "Epoch 12/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3137 - accuracy: 0.8870"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 16s 2s/step - loss: 0.3137 - accuracy: 0.8870\n",
            "Epoch 13/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3597 - accuracy: 0.8780"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 15s 2s/step - loss: 0.3597 - accuracy: 0.8780\n",
            "Epoch 14/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3507 - accuracy: 0.8840"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 16s 2s/step - loss: 0.3507 - accuracy: 0.8840\n",
            "Epoch 15/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.8660"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 15s 2s/step - loss: 0.3378 - accuracy: 0.8660\n",
            "Epoch 16/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.8824"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 15s 2s/step - loss: 0.3302 - accuracy: 0.8824\n",
            "Epoch 17/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.8710"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 15s 2s/step - loss: 0.3570 - accuracy: 0.8710\n",
            "Epoch 18/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3053 - accuracy: 0.8856"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 15s 1s/step - loss: 0.3053 - accuracy: 0.8856\n",
            "Epoch 19/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.8860"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 16s 2s/step - loss: 0.3046 - accuracy: 0.8860\n",
            "Epoch 20/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2356 - accuracy: 0.9180"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 16s 2s/step - loss: 0.2356 - accuracy: 0.9180\n"
          ]
        }
      ],
      "source": [
        "# El código aquí\n",
        "optim = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model = Model(inputs=conv_base.input, outputs=output_layer)\n",
        "model.compile(optimizer=optim,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "xception_history = model.fit(traingen,\n",
        "                                  batch_size=100,\n",
        "                                  epochs=20,\n",
        "                                  validation_data=validgen,\n",
        "                                  steps_per_epoch=10,\n",
        "                                  validation_steps=10,\n",
        "                                  callbacks=[es_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lqcV0nzNRYY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}